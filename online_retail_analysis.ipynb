{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Source: https://www.kaggle.com/datasets/lakshmi25npathi/online-retail-dataset\n",
    "# Folder: Online Retail Sales\n",
    "# Description:\n",
    "##This Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.\n",
    "##The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Online retail sales dataset\n",
    "\n",
    "## Overall goals:\n",
    "- See the shape of the dataset and explore it\n",
    "- Take a quick look at the data and decide on what to focus\n",
    "- Manipulate data(changing data, creating new data, cleaning data etc...)\n",
    "- Visualize findings to tell a story and back up analysis conducted\n",
    "    - Visualizations include:\n",
    "        - Geographical map\n",
    "        - Bar Graph, Histogram, Scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used throughout\n",
    "- Pandas\n",
    "- Plotly\n",
    "- NumPy\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the case of errors\n",
    "- Not all python libraries may be on your machine and or within your directory. Ensure to install them.\n",
    "- You ran a cell with an edit that you made to it(This notebook is designed to run seamlessly with no edits)\n",
    "- Not running a python kernel or you're using an old version of python kernel\n",
    "- Don't have libraries that are necessary for operation of parts or the entirety certain libraries.\n",
    "    - Ex. nbformat is needed for certain features of the plotly library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries to be used\n",
    "# pip install \"name of library\"(incase there is an error where the library is requested or not identified by your system)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the excel file to csv to create the main DataFrame\n",
    "online_retail = pd.read_excel('Online_Retail.xlsx')\n",
    "online_retail.to_csv('Online_Retail.csv', index=False)\n",
    "online_retail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick look of the DataFrame\n",
    "online_retail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Information:\n",
    "- InvoiceNo: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n",
    "- StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n",
    "- Description: Product (item) name. Nominal.\n",
    "- Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "- InvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\n",
    "- UnitPrice: Unit price. Numeric. Product price per unit in sterling (Â£).\n",
    "- CustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n",
    "- Country: Country name. Nominal. The name of the country where a customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will want to see what is unique about each of the qualitative columns\n",
    "#### This will allow us to get an idea of distinction within the dataset where it matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizing NumPy to find the count of unique values in the description column\n",
    "description = online_retail['Description'].unique().tolist() #tolist isnt necessary but for safety it stores the array of values to a list\n",
    "print(type(description))\n",
    "len(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_id = online_retail['CustomerID'].unique().tolist() #tolist isnt necessary but for safety it stores the array of values to a list\n",
    "print(type(cust_id))\n",
    "len(cust_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = online_retail['Country'].unique().tolist() #tolist isnt necessary but for safety it stores the array of values to a list\n",
    "print(type(country))\n",
    "len(country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to see if there are any null values within the data and analyze them to see whether they should be deleted or changed, or a mix of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_retail[online_retail.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interestingly 135080 rows/541909 rows have null values. There is a lot of cleaning to be done.\n",
    "#Lets check what columns have null values\n",
    "online_retail.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we can see that the focus is to be placed on the Description and CustomerID column.\n",
    "#From this its possible to infer that maybe its orders that never went through or had errors. Lets check to see the counts of null values for each\n",
    "online_retail['Description'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for customerID\n",
    "online_retail['CustomerID'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall more customerID rows are null in comparison to description\n",
    "### Additionally, I've noticed that there are rows of data where the unit price is listed as 0.\n",
    "- This isn't null, but to me I view it as that. Therefore, I will clean this data to remove rows with these conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First find all the rows\n",
    "cust_desc_errors = online_retail[online_retail['Description'].isnull() & online_retail['CustomerID'].isnull()]\n",
    "cust_desc_errors\n",
    "\n",
    "\n",
    "#new_online_retail = online_retail[online_retail[[cust_desc_errors & ]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we delete those rows which is essentially the opposite of our previous operation but we change from using & and use the or condition\n",
    "new_online_retail = online_retail[online_retail['Description'].notnull() | online_retail['CustomerID'].notnull()]\n",
    "new_online_retail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have a slightly more cleaned up df which now has the issue with the null descriptions gone.\n",
    "### We'll now sort the issue with the null customerID values. Note that its possible they may be guest purchases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_online_retail.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going forward, I'm gonna conduct some tests on other columns to find places where there may be errors like a negative quantity and also remove them as well\n",
    "new_online_retail = new_online_retail[new_online_retail['Quantity'] > 0]\n",
    "new_online_retail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next I'll check the unique values in each column that we arent sure of if there are valid values for\n",
    "new_online_retail['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick check on the uncspecified country to see if anything is wrong from first glance\n",
    "new_online_retail[new_online_retail['Country'].str.contains('Unspecified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the countries have no issues, however, there is EIRE which is actually Ireland and RSA which is actual South Africa.\n",
    "#Therefore I will replace it in the df so it makes more sense to someone who checks the new df and any visualizations on countries\n",
    "new_online_retail['Country'].replace({'EIRE': 'Ireland', 'RSA': 'South Africa'}, inplace=True) #inplace=True just modifies the df rather than creating a new one if it was false\n",
    "new_online_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next I also want to check if there are any prices that are negative.\n",
    "new_online_retail[new_online_retail['UnitPrice']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We find that there is. And we do not want to keep this in our df.\n",
    "#However we do want to keep it stored in a variable for future reference since it is a bad debt\n",
    "bad_debt = new_online_retail['UnitPrice'] < 0\n",
    "new_online_retail = new_online_retail.drop(new_online_retail[bad_debt].index)\n",
    "new_online_retail.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have now cleaned up the dataset pretty well.\n",
    "- Removed a few null values from various fields\n",
    "- replaced the values of some fields to be more understandable to any audience\n",
    "- Got rid of rows where the values didn't logically make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again lets check what else has null values\n",
    "new_online_retail.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How much more of these values are null?\n",
    "new_online_retail[new_online_retail['CustomerID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # library for regular expressions\n",
    "\n",
    "# Find all unique stock codes\n",
    "unique_stock_code = new_online_retail['StockCode'].drop_duplicates()\n",
    "\n",
    "# Define the regular expression pattern\n",
    "pattern = r'^[a-zA-Z\\s]*$'\n",
    "\n",
    "# Find stock codes with only words or letters\n",
    "matching_vals = new_online_retail[new_online_retail['StockCode'].str.match(pattern, na=False)]\n",
    "string_stock_codes = matching_vals['StockCode'].unique().tolist()\n",
    "string_stock_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze these further.\n",
    "result_df = pd.DataFrame()  # Initialize an empty DataFrame to store results\n",
    "\n",
    "for stock_code in string_stock_codes:\n",
    "    subset_row = new_online_retail[new_online_retail['StockCode'] == stock_code].head(1)[['StockCode', 'Description']]\n",
    "    result_df = pd.concat([result_df, subset_row], ignore_index=True)\n",
    "\n",
    "# Display the result\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Everything above looks fine except for the stockcodes of m and M\n",
    "M_stock_codes = pd.DataFrame()\n",
    "\n",
    "M_stock_codes = new_online_retail[new_online_retail['StockCode'] == 'M'].head(10)\n",
    "M_stock_codes = pd.concat([M_stock_codes,new_online_retail[new_online_retail['StockCode'] == 'm'].head(10)],ignore_index=True)\n",
    "M_stock_codes.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this we were able to see two things, and it is that the little m was probably an error in the dataset\n",
    "#That will get replaced as capital M.\n",
    "#Also luckily there was only one occurence of this error\n",
    "new_online_retail['StockCode'].replace('m','M', inplace=True)\n",
    "M_stock_codes['StockCode'].replace('m','M', inplace=True)\n",
    "M_stock_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have now cleaned up the dataset very well. And will now move on to analysis that we'll conduct on the countries of the dataset\n",
    "### Some things to notice or consider are the following:\n",
    "- The CustomerID column still contains null values, however there isn't anything that more we can do with it\n",
    "    - The null values may just be guest purchases. Additionally, they are about 10,000 rows worth of data so it is not worth deleting\n",
    "- We will first do some analysis on the US and then create a few visualizations to display findings\n",
    "- We will also create a geographical map that showcases some densities in relation to where customers are from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First is analysis of Sales in the US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will start with creating a new df with only sales from customers in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the unique countries within the new online retail sales df\n",
    "new_online_retail['Country'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the array, above we see US customers are listed as \"USA\"\n",
    "#Now we create the new df\n",
    "us_customers = new_online_retail[new_online_retail['Country'] == 'USA']\n",
    "print(f\"All transactions: {len(new_online_retail)} vs transactions from US customers: {len(us_customers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we can see that the US is a very small percentage of this retail store's us_customers\n",
    "#From this, the owners of the retail store want to focus see two things below.\n",
    "#What items are bought the most and how much money is spent on these items\n",
    "#Lets look at the new df\n",
    "us_customers.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column for the total money spent on an item\n",
    "\n",
    "us_customers.insert(6,'Total spent',us_customers['Quantity'] * us_customers['UnitPrice'])\n",
    "us_customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be used a few cells below but also serves a purpose here\n",
    "unique_us_invoice = us_customers['InvoiceNo'].unique().tolist()\n",
    "unique_us_invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we make a new df that will have the total price per order\n",
    "us_invoice = us_customers[['InvoiceNo','Total spent', 'Country']].groupby(['InvoiceNo','Country']).agg({'Total spent': 'sum'}).reset_index()\n",
    "us_invoice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to hold all the values for the x-axis\n",
    "i=0\n",
    "X = []\n",
    "while i < 5:\n",
    "    X.append(f\"US order #{i+1}\")\n",
    "    i += 1\n",
    "\n",
    "#Y values\n",
    "Y = us_invoice['Total spent'].tolist()\n",
    "Y = [round(val,2) for val in Y]\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple bar graph to showcase the different order sizes\n",
    "plt.bar(X,Y)\n",
    "plt.xlabel(\"US orders\")\n",
    "plt.ylabel(\"Money per order\")\n",
    "plt.yticks(Y,[f'${values}' for values in Y])\n",
    "plt.title(\"US orders made\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_order1, us_order2, us_order3, us_order4, us_order5 = [us_customers[us_customers['InvoiceNo'] == val].reset_index(drop=True) for val in unique_us_invoice]\n",
    "#example of one of the df with data for one unique invoiceNo\n",
    "us_order1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_order1.index.tolist()\n",
    "us_order1['Total spent'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the automation I have set for the graphs\n",
    "#So this is a good first view of just how much was spent amongst the orders from customer(s) in the US.\n",
    "#We see that there is decent variability and but most orders are around the range of $500 dollars excluding the extremely large purchase of 1579 total\n",
    "#Next we want to see how the actual distribution of each unique item compares with each order and all orders together\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5,ax6)) = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))\n",
    "\n",
    "\n",
    "\n",
    "#The subplots and dataframes are in lists that will be used within a for loop\n",
    "graphs = [ax1,ax2,ax3,ax4,ax5]\n",
    "orders = [us_order1, us_order2, us_order3, us_order4, us_order5]\n",
    "colors=['red', 'green', 'blue', 'purple', 'orange']\n",
    "\n",
    "#for loop to create all the scatter plots for comparisons\n",
    "for graph, order,colour in zip(graphs, orders, colors):\n",
    "    #common labels\n",
    "    graph.set_xlabel('Order',fontsize=10)\n",
    "    graph.set_ylabel('Total spent per item',fontsize=10)\n",
    "\n",
    "\n",
    "    # Scatter plot\n",
    "    graph.axis([-5, 200, -5, 100])\n",
    "    graph.scatter(order.index.tolist(), order['Total spent'].values, color = colour)\n",
    "\n",
    "    #adding dollar signs to the y ticks\n",
    "    # Set the y-axis ticks with dollar signs\n",
    "    y_ticks = graph.get_yticks()\n",
    "    graph.set_yticklabels([f'${y}' for y in y_ticks])\n",
    "\n",
    "\n",
    "#last graph that includes all orders together\n",
    "ax6.set_xlabel('All orders',fontsize=10)\n",
    "ax6.set_ylabel('Total spent per item',fontsize=10)\n",
    "ax6.set_title('All orders together',fontsize = 15)\n",
    "\n",
    "# scatter plot\n",
    "ax6.axis([-5, 200, -5, 100])\n",
    "ax6.scatter(us_customers.reset_index(drop=True).index.tolist(), us_customers['Total spent'].values, color = 'cyan')\n",
    "\n",
    "#set y ticks again\n",
    "ax6_yticks = ax6.get_yticks()\n",
    "ax6.set_yticklabels(f'${a}' for a in ax6_yticks)\n",
    "\n",
    "#Figure titling and then displaying everything\n",
    "fig.suptitle(\"Comparison of distribution per unique items in each order\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will finally end analysis and visualizations off with a map showcasing some information on customers/sales for all countries within the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geographic data to create the map for the countries with choropleth library\n",
    "country_geo = 'geo/world-countries.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check again for unique countries in the adjusted dataset one more time\n",
    "new_online_retail['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtract the columns we need for the map, invoiceNo and Country\n",
    "country_data = new_online_retail[['Country','InvoiceNo']]\n",
    "country_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new DataFrame with unique pairs of 'Country' and 'InvoiceNo'\n",
    "unique_country_data = country_data.drop_duplicates()\n",
    "\n",
    "\n",
    "# Get the count of occurrences for each 'Country' across all 'InvoiceNo'\n",
    "country_counts = unique_country_data['Country'].value_counts().reset_index()\n",
    "country_counts.columns = ['Country', 'Count']\n",
    "\n",
    "#adjusting unique_country_data to meet a field within the json data we will be using in the map a few cells down\n",
    "#essentially USA is the abbreviation/id and not the actual name. Every other country uses its full name\n",
    "country_counts['Country'].replace(\"USA\",\"United States of America\", inplace=True)\n",
    "country_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Interactive density map\n",
    "fig = px.choropleth(\n",
    "    country_counts,\n",
    "    geojson=country_geo,\n",
    "    locations='Country',\n",
    "    featureidkey='properties.name',\n",
    "    color='Count',\n",
    "    color_continuous_scale='Viridis',\n",
    "    range_color=(country_counts['Count'].min(), country_counts['Count'].max()),\n",
    "    title='Pop Density of customers',\n",
    "    labels={'Count': 'Customer Count'},\n",
    "    width=1200,\n",
    "    height=900\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we see that this graph, isnt very useful because the UK is a huge outlier given that the retail store is located and based in UK.\n",
    "# Therefore we will create one last df that removes the UK and only focuses on every other country\n",
    "\n",
    "every_other_country = country_counts[country_counts['Country'] != 'United Kingdom']\n",
    "every_other_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive density map updated\n",
    "fig = px.choropleth(\n",
    "    every_other_country,\n",
    "    geojson=country_geo,\n",
    "    locations='Country',\n",
    "    featureidkey='properties.name',\n",
    "    color='Count',\n",
    "    color_continuous_scale='Oranges', #used blues to keep the shading consistent. So darker means more customers\n",
    "    range_color=(-20, every_other_country['Count'].max()),\n",
    "    title='Pop Density of customers',\n",
    "    labels={'Count': 'Customer Count'},\n",
    "    width=1200,\n",
    "    height=900\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This map is shows more variation amongst the countries even though there isn't as much outside of the countries that aren't neighbors to the UK except for a few. \n",
    "### Overall, the map allows for easy understanding of what countries that the UK retail store should or could focus sales, advertising, marketing etc... to"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
